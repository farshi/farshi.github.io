<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[FARSHI DESIGNS]]></title>
  <link href="http://www.farshidesigns.com/atom.xml" rel="self"/>
  <link href="http://www.farshidesigns.com/"/>
  <updated>2015-04-28T15:22:36+10:00</updated>
  <id>http://www.farshidesigns.com/</id>
  <author>
    <name><![CDATA[Reza Farshi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Amazon S3 Notes]]></title>
    <link href="http://www.farshidesigns.com/blog/2015/04/27/amazon-s3-notes/"/>
    <updated>2015-04-27T22:48:19+10:00</updated>
    <id>http://www.farshidesigns.com/blog/2015/04/27/amazon-s3-notes</id>
    <content type="html"><![CDATA[<p>Some Notes about Amazon Simple Storage Service (S3) service , you will find it useful if you want to prepare for AWS architecture certificate.</p>

<p>Amazon Simple Storage Service (S3) :</p>

<ul>
<li>Files can be from 1 Byte to 5 TB</li>
<li>Files are stored in Buckets and there is no limit.</li>
<li>Buckets name must be unique in a region level</li>
<li>Buckets are private by default</li>
<li>S3 Files are stored as a object in buckets</li>
<li>S3 supports MultiPart uploads,It is necessary for files bigger than 5Gb</li>
<li>S3 support resumable uploads.</li>
<li>S3 is consistent cross availability zones</li>
<li>S3 has a Life Cycle management, you can archive data after a specific time period from upload time in glacier.</li>
<li>Amazon S3 designed for 11 nines durability , 99.99999999999 in a given year</li>
<li>Amazon S3 availability is 99.99% in a given  year</li>
<li>Files can be encrypted on S3</li>
<li>S3 has versioning feature , includes all writes even if you delete an object</li>
<li>When you enable the versioning , you can not disable it , but you can suspend it</li>
<li>Reduced Redundancy Storage (RSS) data availability and durability is 99.99%</li>
<li>RSS is suitable for storing low frequency data.</li>
<li>RSS Cost: 3 Cent per GB per Month</li>
<li>Glacier is a low cost storage service suitable for data archiving.</li>
<li>Glacier Cost: 1 Cent per GB per Month</li>
<li>Glacier retrieval time 3 to 5 Hours</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Amazon SNS Notes]]></title>
    <link href="http://www.farshidesigns.com/blog/2015/04/27/amazon-sns-notes/"/>
    <updated>2015-04-27T22:10:57+10:00</updated>
    <id>http://www.farshidesigns.com/blog/2015/04/27/amazon-sns-notes</id>
    <content type="html"><![CDATA[<p>Some Notes about Amazon SMS service , you will find it useful if you want to prepare for AWS architecture certificate.</p>

<p>Amazon Simple Notification service  (SNS) :</p>

<ul>
<li>Is a web service that makes it easy to set up, operate, and send notifications from the cloud.</li>
<li>Amazon SNS allows the &ldquo;publish-subscribe&rdquo; (pub-sub) messaging paradigm.</li>
<li>SNS eliminates the need to periodically check or pull for new information and updates.</li>
<li>it is inexpensive, pay-as-you-go model with no up-front costs.</li>
<li>Amazon SNS can be deliver notifications by SMS text messages or email, to Amazon Simple Queue Service (SQS) queues, or any HTTP endpoint.</li>
<li>To prevent messages from being lost,all messages published to Amazon SNS are stored redundantly across multiple availability zones.</li>
<li>SNS Topic allows you to group multiple recipients. It acts like a access point for allowing recipients to dynamically subscribe for identical copies of the same notification.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Amazon SWF Notes]]></title>
    <link href="http://www.farshidesigns.com/blog/2015/04/27/amazon-swf-notes/"/>
    <updated>2015-04-27T22:10:17+10:00</updated>
    <id>http://www.farshidesigns.com/blog/2015/04/27/amazon-swf-notes</id>
    <content type="html"><![CDATA[<p>Some Notes about Amazon SWF service , you will find it useful if you want to prepare for AWS architecture certificate.</p>

<p>Amazon Simple Work flow Service (SWF):</p>

<ul>
<li>is a web service that makes it easy to coordinate work across distributed application components.</li>
<li>enables applications for a range of use cases, including media processing, web application back-ends, business process work flows, and analytics pipelines.</li>
<li>Tasks represent invocations of various processing steps in an application which can be performed by executable code, web service calls, human actions and scrips.</li>
<li>SWF presents a task-oriented API</li>
<li>SWF ensures that a task is assigned only once and is never duplicated.</li>
<li>SWF keeps track of all the tasks and events in an application.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Amazon SQS Notes]]></title>
    <link href="http://www.farshidesigns.com/blog/2015/04/27/amazon-sqs/"/>
    <updated>2015-04-27T22:09:04+10:00</updated>
    <id>http://www.farshidesigns.com/blog/2015/04/27/amazon-sqs</id>
    <content type="html"><![CDATA[<p>Some Notes about Amazon SQS service , you will find it useful if you want to prepare for AWS architecture certificate.</p>

<p>Amazon Simple queue service (SQS):</p>

<ul>
<li>is A web service that gives you access to a message queues.</li>
<li>Queues can be used to store messages while waiting for a computer to process them.</li>
<li>is distributed Queue System.</li>
<li>is fast and reliable</li>
<li>A queue is a temporary repository for messages hat are waiting processing.
*By using SQS you can decouple the components of an application so they run independently.</li>
<li>Messages can contain up to 256KB of text in any format.</li>
<li>Messages can be retrieved by any distributed component by using Amazon SQS API.</li>
<li>Amazon SQS ensures delivery of each message at least once. so one message can be read by many distributed application components simultaneously. And They are remained until you remove them from the queue yourself.</li>
<li>SQS does not support FIFO ( First In Firs Out).</li>
<li>SQS is a pull system , and messages are not pushed out like SNS.</li>
<li>Visibility time out period for a message starts when a component retrieves the message. The default visibility time out is minimum 30 seconds and maximum 12 hours.</li>
<li>SQS Billed at 64kb Chunks.</li>
<li>Firs 1 million Amazon SQS request per month are free.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Amazon VPC Notes]]></title>
    <link href="http://www.farshidesigns.com/blog/2015/04/27/amazon-vpc/"/>
    <updated>2015-04-27T22:08:11+10:00</updated>
    <id>http://www.farshidesigns.com/blog/2015/04/27/amazon-vpc</id>
    <content type="html"><![CDATA[<p>Some Notes about Amazon VPC service , you will find it useful if you want to prepare for AWS architecture certificate.</p>

<p>Amazon VPC:</p>

<ul>
<li>It is like a virtual data center in cloud.</li>
<li>lets you provision a logically isolated sections of the Amazon web service (AWS) cloud.</li>
<li>you have complete control over your virtual network environment.</li>
<li>you can select your own IP address range</li>
<li>you can crate your subnets</li>
<li>you can configure route tables</li>
<li>you can setup network gateways ( Internet gateway and Customer gateway).</li>
<li>With VPC you have two level of security groups:

<ul>
<li>  instance security groups.</li>
<li>  subnet network access control lists (ACLS)</li>
</ul>
</li>
<li>using Amazon Direct Connect you can connect your corporate data center and your AWS VPC.</li>
<li>There is a default VPC which is user friendly, allowing you to immediately deploy instances.</li>
<li>All Subnets in default VPC have an Internet gateway attached.</li>
<li>Each EC2 instance has both a public and private IP address.</li>
<li>If you delete the default CVPC the only way to get it back is to contact AWS.</li>
<li>you can user VPC Peering to connect one VPC with another.</li>
<li>when you use VPC Peering , instances behave as if they were on the same private network.</li>
<li>you can peer even VPC&rsquo;s with other AWS accounts.</li>
<li>Peering is in a star configuration.So Siblings VPC&rsquo;s can not directly talk to each other and should use centric one.</li>
<li>Elastic IP addresses, Internet Gateways, VPCs per region : Max 5</li>
<li>VPC connections per region, Customer Gateways per region: Max 50</li>
<li>Route tables per region: Max 200</li>
<li>Security groups per VPC: Max 100</li>
<li>Rules per security group: Max 50</li>
<li>Dedicated VPC&rsquo;s are expensive because all of instance underneath will be dedicated.</li>
<li>When you create a VPC, one route table and one ACL will be created automatically.</li>
<li>Your subnets can not be in different availability zones , just in one AZ.</li>
<li>you can have only One Internet gateway per VPC.</li>
<li>putting an instance in a public subnet is not enough to make public access to that instance.you should assign public IP address to it to make it accessible from the Internet.</li>
<li>For making a NAT instance as a router for routing private subnet traffic to the Internet you should disable source/destination check for that NAT instance.</li>
<li>Access Control list act as firewall and rules are applied to the entire subnet.
*If you don&rsquo;t associate a ACL to a subnet, default ACL which allows all incoming and outgoing traffic will be assigned to a subnet automatically.</li>
<li>Rules in ACL assigned order numbers, so rule #100 evaluated before rule #200. Amazon recommends to increment rules number by 100.</li>
<li>When you create a new ACL, incoming and outgoing traffic is denied by default.</li>
<li>Like as a firewall , Only one ACL can be assigned to the subnet.cardinality between ACL-Subnet is Many to one.</li>
<li>Amazon corporate network is segregated from AWS network.</li>
<li>Port scanning is not allowed in AWS by default. for vulnerability scan you must request it from amazon for a short period.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Amazon RedShift Notes]]></title>
    <link href="http://www.farshidesigns.com/blog/2015/04/27/amazon-redshift-notes/"/>
    <updated>2015-04-27T22:06:33+10:00</updated>
    <id>http://www.farshidesigns.com/blog/2015/04/27/amazon-redshift-notes</id>
    <content type="html"><![CDATA[<p>Some Notes about Amazon RedShift service , you will find it useful if you want to prepare for AWS architecture certificate.</p>

<p>Amazon RedShift is the Amazon warehouse service in cloud, characteristics are :</p>

<ul>
<li>fast and powerful</li>
<li>fully managed service</li>
<li>Peta byte scale</li>
<li>Instead of storing data as a series of rows, it has columnar data storage</li>
<li>Column-based storage require far fewer I/Os so greatly improving query performance</li>
<li>10 Time Faster than traditional data warehouses.</li>
<li>in column-base storage , similar data is stored sequentially , so data can be compressed much more than row-base storage,</li>
<li>RedShift automatically samples your data and select the most appropriate compression schema.</li>
<li>Redshift has MPP ( Massively Parallel Processing) feature.</li>
<li>by MPP , data are distributed and queries load across all nodes to your data</li>
<li>cost is less than a tenth of most other data warehouse solutions.</li>
<li>configuration of RedShif can be Single node(160Gb) or Multi-node.</li>
<li>in Multi-Node mode , One node is Leader node , and then you can create  up to 128 compute nodes.</li>
<li>Leader node manages client connections and receives queries</li>
<li>Computed Node store data and perform queries and do computations</li>
<li>You will not charged for leader node hours; only compute nodes will incur charges.</li>
<li>Encrypted in transit using SSL.</li>
<li>Encrypted at rest using AES-256</li>
<li>By default Redshift takes care of key management</li>
<li>You can manage your own keys through HSM or by Amazon Key Management Service.</li>
<li>Currently only available in 1 AZ</li>
<li>Can restore snapshots to new AZ&rsquo;s in the event of outage.</li>
<li>Fail over is not automatically , it&rsquo;s manual.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Amazon DynamoDB Notes]]></title>
    <link href="http://www.farshidesigns.com/blog/2015/04/27/amazon-dynamodb-notes/"/>
    <updated>2015-04-27T22:05:47+10:00</updated>
    <id>http://www.farshidesigns.com/blog/2015/04/27/amazon-dynamodb-notes</id>
    <content type="html"><![CDATA[<p>Some Notes about Amazon DynamoDB service, you will find it useful if you want to prepare for AWS architecture certificate.</p>

<ul>
<li>Amazon DynamoDb is a fast and flexible NoSQL database service.</li>
<li>single digit millisecond latency at any scale</li>
<li>fully managed</li>
<li>supports both document and key-value data</li>
<li>It&rsquo;s flexible data model and reliable performance make it a great fit for mobile, web, gaming, ad-tech, IoT, and many other applications.</li>
<li>Stored on SSD storage</li>
<li>spread across 3 geographically distinct data centres</li>
<li>It supports both of Eventual Consistency and Strong Consistency.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Amazon RDS Notes]]></title>
    <link href="http://www.farshidesigns.com/blog/2015/04/27/amazon-rds-notes/"/>
    <updated>2015-04-27T22:04:28+10:00</updated>
    <id>http://www.farshidesigns.com/blog/2015/04/27/amazon-rds-notes</id>
    <content type="html"><![CDATA[<p> Some Notes about Amazon RDS service, you will find it useful if you want to prepare for AWS architecture certificate.</p>

<ul>
<li>Amazon RDS now supports these Relational (OLTP): MySQL, Oracle , SQL , PostgreSql, Aurora</li>
<li>Supports Non-Relational No-SQL database , DynamoDB</li>
<li>Supports Data warehousing database (OLAP), RedShift</li>
<li>Supports ElasticCache (fast in-memory cache service), MemCached and Redis</li>
<li>Aurora is a MySql Compatible Database , Not accessible yet.</li>
<li>Aurora provides up to five times better performance than MySQL.</li>
<li>Aurora cost is one tenth that of a commercial database while delivering similar performance and availability.</li>
<li>Aurora start with 10Gb, Scales in 10 Gb increments to 64Tb.</li>
<li>compute resources can scale up to 32vCPUs and 244 Gb of Memory.</li>
<li>Aurora storage is self-healing. Data blocks scanned and fixed automatically.</li>
<li>you can loose 2 copies without effecting write performance.</li>
<li>you can loose 3 copies without effecting read performance.</li>
<li>Maximum Backup Retention Period for RDS is 35 days</li>
<li>When you want restore database to specific time , entirely new database will be created and endpoints will be different from the original database endpoint</li>
<li>you can bring your own license in the case of selecting Oracle and SQL Server as a RDS Engine.</li>
<li>You can modify MySql DB instance and increase storage size , change engine , change database name , password and &hellip;</li>
<li>Microsoft SQL server DB instance has some restrictions:</li>
<li> You can not modify Microsoft SQL server DB instance storage size , Amazon does not support increasing storage on a SQL server DB Instance.</li>
<li> A newly created SQL server does not contain database. you should connect from SQL server management studio and create your database.</li>
<li>The maximum storage size for SQL server DB instance is 1024 GB.</li>
<li>Read replicas are available in Amazon RDS for MySQL, PostgreSQL, and Amazon Aurora.</li>
<li>Amazon RDS for MySQL and PostgreSQL allow you to add up to 5 read replicas to each DB Instance.</li>
<li>With Aurora you will get 6 copies of your data (2 copies in each 3 AZ).</li>
<li>When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[EC2 Notes]]></title>
    <link href="http://www.farshidesigns.com/blog/2015/04/25/ec2-essentials/"/>
    <updated>2015-04-25T12:27:57+10:00</updated>
    <id>http://www.farshidesigns.com/blog/2015/04/25/ec2-essentials</id>
    <content type="html"><![CDATA[<p>Some Notes about Elastic compute cloud , you will find it useful if you want to prepare for AWS architecture certificate.</p>

<ul>
<li>Elastic Compute Cloud provides re-sizable compute capacity in cloud.</li>
<li>It can be scale up or down as computing requirements changed.</li>
<li>EC2 Options: Free Tier, On Demand , Reserved and Spot.</li>
<li>Data Stored on a local instance storage will be deleted when you stop the instance.</li>
<li>EBS backed storage will persist independently of the life of the instance</li>
<li>AWS uses standard technologies detailed in DoD 5220.22-M or NIST 800-88 guidelines for media sanitization to destroy data as part of the decommissioning process.</li>
<li>EBS volume can be attached to only One instance at a time.</li>
<li>Types of storage backed by EBS:

<ul>
<li>  General purpose SSD , 99.999% availability,</li>
<li>  Provisioned IOPS SSD, Designed for IO intensive applications</li>
<li>  Magnetic, Low cost , Suitable for infrequent accessed data</li>
</ul>
</li>
<li>Identity Access Management (IAM) roles can not be assigned to an EC2 instance after the instance has been created.</li>
<li>You can not add or change roles to an existing EC2 instance but you can change the permissions of a assigned role.</li>
<li>EC2 placement group properties:

<ul>
<li>  group of logical groups of instances in only one singe AZ</li>
<li>  Are low latency ; has 10GBbps network throughput.</li>
<li>  The name you choose for placement group should be unique in AWS account</li>
<li>  Same size instances recommended to be put in a group</li>
<li>  placement groups cannot be merged.</li>
<li>  you can add existing instances to your placement group by creating AMI form that instance and then launch it to your placement group</li>
</ul>
</li>
<li>Lambda is a compute service which run a code in response of events occurs in your resources and automatically manage that resource for you.</li>
<li>Responses of lambda including capacity provisioning, OS and servers maintenance, security patch , logging and &hellip;</li>
<li>Supported programming language for lambda is JavaScript.</li>
<li>Lambda designed to be available in 99.99%</li>
<li>First one million request of Lambda are free and there after is 0.20 per 1 mil request</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Amazon AWS Numerical Facts]]></title>
    <link href="http://www.farshidesigns.com/blog/2015/04/24/amazon-aws-numbers-and-default-limits-facts/"/>
    <updated>2015-04-24T16:01:40+10:00</updated>
    <id>http://www.farshidesigns.com/blog/2015/04/24/amazon-aws-numbers-and-default-limits-facts</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve collected some information about default limits in AWS and other numerical facts from <a href="http://aws.amazon.com/">Amazon AWS official website</a>. May be you find it useful if you want to prepare for AWS architecture certification. This post my be updated in the future.</p>

<ul>
<li>AWS is available in : 190 countries</li>
<li>Number of available regions: 11</li>
<li>Number of Edge Locations: 52</li>
<li>Availability zones per regions: two or more</li>
<li>Amazon S3 designed for 11 nines durability , 99.99999999999% in a given year</li>
<li>Amazon S3 availability is 99.99% in a given year</li>
<li>Objects size range in S3  is :  Min: 1 Byte , Max: 5TB</li>
<li>S3 Number of buckets: 100</li>
<li>Number of objects you can store: UNLIMITED</li>
<li>Glacier retrieve time : 3 to 5 Hours</li>
<li>RSS type durability is %99.99 instead Standard S3 which is %99.99999999999</li>
<li>RDS size range : Min: 5GB , Max:3TB</li>
<li>Aurora size range : Min: 10GB  Max: 64TB</li>
<li>Aurora Availability : 2 Copies for each 3 Zone &ndash;> 6 Copies</li>
<li>Elastic Block Storage (EBS) size , Min: 1GB  , Max: 1TB</li>
<li>VPC per region Max : 5 (default - increased upon request)</li>
<li>Internet Gateway per region : Max 5 (default - increased upon request)</li>
<li>Customer Gateways per region : Max 50 (default - increased upon request)</li>
<li>Elastic IP addresses per region: Max 5 (default - increased upon request)</li>
<li>Rout tables per VPC : Max 200 (default - increased upon request)</li>
<li>Internet Attached to a subnet at time : Only 1</li>
<li>Security group per VPC : Max 100</li>
<li>Rules per Security Group: Max 50</li>
<li>VPN connections per region: Max 50</li>
<li>General purpose SSD EBS availability: 99.999%</li>
<li>General purpose SSD EBS IOPS per GB: 3 to 3000 ( just for short period)</li>
<li>Simple Queue Service(SQS) Message Size : Max 256 Kb</li>
<li>AWS CloudFormaton Limits : 20 stack</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sublime Text as a Default Editor for Git and Others]]></title>
    <link href="http://www.farshidesigns.com/blog/2015/04/16/sublime-text-as-a-default-editor-for-git-and-others/"/>
    <updated>2015-04-16T13:37:09+10:00</updated>
    <id>http://www.farshidesigns.com/blog/2015/04/16/sublime-text-as-a-default-editor-for-git-and-others</id>
    <content type="html"><![CDATA[<p>If you want to set sublime as the main editor for git and other programs on your mac, depends on what you bash you use you should edit  ~/.bash_profile or ~/.bashrc or ~/.zshrc and put these lines on it:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export VISUAL='subl -w'
</span><span class='line'>export EDITOR="$VISUAL"
</span><span class='line'>export GIT_EDITOR="$VISUAL"
</span></code></pre></td></tr></table></div></figure>


<p>Then open a new terminal and run this command :</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ln -s "/Applications/Sublime Text 2.app/Contents/SharedSupport/bin/subl" ~/bin/subl</span></code></pre></td></tr></table></div></figure>


<p> Now sublime can be run as a console program and when you hit <code>git commit</code> command sublime shows up in a new Tab. Refer to <a href="https://www.sublimetext.com/docs/2/osx_command_line.html">Sublime documentation</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installation Tips - Octopress Over GitHubPages.]]></title>
    <link href="http://www.farshidesigns.com/blog/2015/04/15/an-installation-tips-octopress-over-githubpages/"/>
    <updated>2015-04-15T13:43:48+10:00</updated>
    <id>http://www.farshidesigns.com/blog/2015/04/15/an-installation-tips-octopress-over-githubpages</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve recently setup my weblog  using Octopress weblog framework over githubpages. I noticed some delicate hints should be applied when setting up Octopress over GitHubPages. When you are trying to install the Octopress weblog framework over your GitHubPages, according to the Octopress installation guide you should fist login into your GitHub account and create a repository and  name the repository with the format username.github.io, where username is your GitHub user name or organization name. now some tips to prevent getting errors like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>c## Pushing generated _deploy website
</span><span class='line'>To git@github.com:farshi/farshi.github.io.git
</span><span class='line'> ! [rejected]        master -&gt; master (non-fast-forward)
</span><span class='line'>error: failed to push some refs to 'git@github.com:farshi/farshi.github.io.git'
</span><span class='line'>To prevent you from losing history, non-fast-forward updates were rejected
</span><span class='line'>Merge the remote changes (e.g. 'git pull') before pushing again.  See the
</span><span class='line'>'Note about fast-forwards' section of 'git push --help' for details.</span></code></pre></td></tr></table></div></figure>


<p>Tip 1: In the page of creating the repository  you should not select &lsquo;Initialize this repository with a README&rsquo;. Unless when you hit rake deploy command you will get this error.</p>

<p>Tip 2: After creating repository you should specify your repository to octopress by hitting this command &lsquo;rake setup&hellip; &rsquo; , when you asked to enter the URI for your repository , stick to the https URL instead of git@ URI.</p>

<p>I hope it helps you to have a smooth Octopress setup.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Finally I Decided to Blog]]></title>
    <link href="http://www.farshidesigns.com/blog/2015/04/14/finally-i-decided-to-blog/"/>
    <updated>2015-04-14T01:45:43+10:00</updated>
    <id>http://www.farshidesigns.com/blog/2015/04/14/finally-i-decided-to-blog</id>
    <content type="html"><![CDATA[<p>Every time I was reading some professional hackers blogs, I thought to myself how good it would be if I had my own weblog and  streamed my thoughts through it.</p>

<p>Today I&rsquo;m so happy and proud of myself coz finally I decided to create my own weblog. I believe in this:</p>

<blockquote><p>Do the difficult things while they are easy and do the great things while they are small. A journey of a thousand miles must begin with a single step.</p><footer><strong>Lao Tzu</strong></footer></blockquote>


<p></p>
]]></content>
  </entry>
  
</feed>
